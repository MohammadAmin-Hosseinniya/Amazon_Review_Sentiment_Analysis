{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc0d2f7-b796-4128-ba8f-467a418f513c",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl;text-align: center;line-height:200%;font-family:vazir;font-size:medium;color:black\"><font face=\"vazir\" size=10><i>\n",
    "Install essentional libraries and modules\n",
    "</i></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea450e81-d0ee-4684-8653-39bd0c6ce3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 477 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 715 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[K     |████████████████████████████████| 776 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 5.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.3.2)\n",
      "Installing collected packages: click, regex, tqdm, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.10.3 tqdm-4.66.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 485 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 38.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 29.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Downloading tokenizers-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 30.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 34.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[31mERROR: tokenizers 0.14.1 has requirement huggingface_hub<0.18,>=0.16.4, but you'll have huggingface-hub 0.19.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: fsspec, huggingface-hub, safetensors, tokenizers, transformers\n",
      "Successfully installed fsspec-2023.10.0 huggingface-hub-0.19.0 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: gdown, PySocks\n",
      "Successfully installed PySocks-1.7.1 gdown-4.7.1\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 554 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.19.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.31.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets>=2.0.0\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[K     |████████████████████████████████| 493 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (23.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.66.1)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.24.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[K     |████████████████████████████████| 266 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: dill, multiprocess, pyarrow, frozenlist, aiosignal, async-timeout, multidict, yarl, aiohttp, xxhash, datasets, responses, evaluate\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.6 dill-0.3.7 evaluate-0.4.1 frozenlist-1.4.0 multidict-6.0.4 multiprocess-0.70.15 pyarrow-14.0.1 responses-0.18.0 xxhash-3.4.1 yarl-1.9.2\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.14.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.19.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.13.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.56.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.16.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's done !! \n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install gdown\n",
    "!pip install evaluate\n",
    "!pip install datasets\n",
    "!pip install tensorflow\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"it's done !! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d542d2-b097-43d8-8f59-17bf9795e955",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl;text-align: center;line-height:200%;font-family:vazir;font-size:medium;color:black\"><font face=\"vazir\" size=10><i>\n",
    "Import Librarys\n",
    "</i></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da805b49-7e5e-4e86-923c-7fdec310cd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-12 12:27:49.658794: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-12 12:27:50.382455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split \n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout ,LeakyReLU\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Dropout, Dense, Input, concatenate\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.initializers import GlorotNormal, HeNormal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import create_optimizer\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d7b3a-f0ee-4faf-81cf-4b0114825aab",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl;text-align: center;line-height:200%;font-family:vazir;font-size:medium;color:black\"><font face=\"vazir\" size=10><i>\n",
    "Download Datasets test and train\n",
    "</i></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7f963b-77c4-452f-b491-37fdf0ef8696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/u/0/uc?id=1-AlW7oNJHaqi3xk_9dWHUS52Dzl_FmFW&export=download\n",
      "From (redirected): https://drive.google.com/uc?id=1-AlW7oNJHaqi3xk_9dWHUS52Dzl_FmFW&export=download&confirm=t&uuid=01e67c0a-8c0e-48ef-9eaa-339ca5b189d4\n",
      "To: /jupyter/train_data.csv\n",
      "100%|██████████| 635M/635M [00:26<00:00, 23.9MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/u/0/uc?id=1-8TsrqTRFP-q9TM-6HinhO0ZVXFHq9TB&export=download\n",
      "To: /jupyter/test_data.csv\n",
      "100%|██████████| 15.6M/15.6M [00:02<00:00, 7.00MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's done !! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url_test = \"https://drive.google.com/u/0/uc?id=1-8TsrqTRFP-q9TM-6HinhO0ZVXFHq9TB&export=download\"\n",
    "url_train = \"https://drive.google.com/u/0/uc?id=1-AlW7oNJHaqi3xk_9dWHUS52Dzl_FmFW&export=download\"\n",
    "\n",
    "gdown.download(url_train , quiet=False)\n",
    "gdown.download(url_test , quiet=False)\n",
    "\n",
    "print(\"it's done !! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcc3ce-ee28-4deb-a0d1-f2769dc2730c",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl;text-align: center;line-height:200%;font-family:vazir;font-size:medium;color:black\"><font face=\"vazir\" size=10><i>\n",
    "Import Data\n",
    "</i></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc0a9f0-40c8-4ea6-af0d-9412c2496e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_429/1570034230.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('train_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data :  (838944, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-11-11</td>\n",
       "      <td>A2OSUEZJIN7BI</td>\n",
       "      <td>0511189877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chris</td>\n",
       "      <td>I have an older URC-WR7 remote and thought thi...</td>\n",
       "      <td>Cannot Learn</td>\n",
       "      <td>1478822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>A2NETQRG6JHIG7</td>\n",
       "      <td>0511189877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Qrysta White</td>\n",
       "      <td>First time I've EVER had a remote that needed ...</td>\n",
       "      <td>zero programming needed!  Miracle!?</td>\n",
       "      <td>1465171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-03-10</td>\n",
       "      <td>A12JHGROAX49G7</td>\n",
       "      <td>0511189877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Linwood</td>\n",
       "      <td>Got them and only 2 of them worked. company ca...</td>\n",
       "      <td>Works Good and programs easy.</td>\n",
       "      <td>1457568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>A1KV65E2TMMG6F</td>\n",
       "      <td>0511189877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dane Williams</td>\n",
       "      <td>I got tired of the remote being on the wrong s...</td>\n",
       "      <td>Same as TWC remote</td>\n",
       "      <td>1452729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>A280POPEWI0NSA</td>\n",
       "      <td>0594459451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kristina H.</td>\n",
       "      <td>After purchasing cheap cords from another webs...</td>\n",
       "      <td>Good Quality Cord</td>\n",
       "      <td>1476921600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall vote  verified  reviewTime      reviewerID        asin style  \\\n",
       "0        2  NaN     False  2016-11-11   A2OSUEZJIN7BI  0511189877   NaN   \n",
       "1        5  NaN      True  2016-06-06  A2NETQRG6JHIG7  0511189877   NaN   \n",
       "2        4  NaN      True  2016-03-10  A12JHGROAX49G7  0511189877   NaN   \n",
       "3        5  NaN      True  2016-01-14  A1KV65E2TMMG6F  0511189877   NaN   \n",
       "4        5  NaN      True  2016-10-20  A280POPEWI0NSA  0594459451   NaN   \n",
       "\n",
       "    reviewerName                                         reviewText  \\\n",
       "0          Chris  I have an older URC-WR7 remote and thought thi...   \n",
       "1   Qrysta White  First time I've EVER had a remote that needed ...   \n",
       "2        Linwood  Got them and only 2 of them worked. company ca...   \n",
       "3  Dane Williams  I got tired of the remote being on the wrong s...   \n",
       "4    Kristina H.  After purchasing cheap cords from another webs...   \n",
       "\n",
       "                               summary  unixReviewTime  \n",
       "0                         Cannot Learn      1478822400  \n",
       "1  zero programming needed!  Miracle!?      1465171200  \n",
       "2        Works Good and programs easy.      1457568000  \n",
       "3                   Same as TWC remote      1452729600  \n",
       "4                    Good Quality Cord      1476921600  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data.csv')\n",
    "print('shape of train_data : ',train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb99609b-cce5-4ffd-adb6-75c8a7dcb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test_data :  (20000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-06-04</td>\n",
       "      <td>A20GGWE66JW9X2</td>\n",
       "      <td>B006Z394GM</td>\n",
       "      <td>{'Color:': ' FPS01-C'}</td>\n",
       "      <td>Brian C Toner</td>\n",
       "      <td>The name and description of this device are mi...</td>\n",
       "      <td>The prize for most useless invention of all ti...</td>\n",
       "      <td>1528070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>ARYJWXHEYHG9M</td>\n",
       "      <td>B005BE058W</td>\n",
       "      <td>{'Size:': ' 1000W', 'Style:': ' G2'}</td>\n",
       "      <td>Snake</td>\n",
       "      <td>One of the molex connectors on the power suppl...</td>\n",
       "      <td>MELTED MOLEX CONNECTOR</td>\n",
       "      <td>1518912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>A10LHZ7WFZ7HLL</td>\n",
       "      <td>B01DA0YCNC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Remote constantly disconnects/ Roku player fre...</td>\n",
       "      <td>Bricked on the regular</td>\n",
       "      <td>1516406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-06-25</td>\n",
       "      <td>A11VN8EOHNLP72</td>\n",
       "      <td>B00FBJ4KYC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Bray</td>\n",
       "      <td>I purchased this 4 year protection plan for a ...</td>\n",
       "      <td>DO NOT BUY!!!</td>\n",
       "      <td>1529884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-08-17</td>\n",
       "      <td>A194Y8P8TVT7P9</td>\n",
       "      <td>B00P7G82TS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mark</td>\n",
       "      <td>I bought one of these and have regretted it ev...</td>\n",
       "      <td>Nightmare - don't buy</td>\n",
       "      <td>1471392000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vote  verified  reviewTime      reviewerID        asin  \\\n",
       "0   NaN      True  2018-06-04  A20GGWE66JW9X2  B006Z394GM   \n",
       "1   NaN      True  2018-02-18   ARYJWXHEYHG9M  B005BE058W   \n",
       "2   NaN      True  2018-01-20  A10LHZ7WFZ7HLL  B01DA0YCNC   \n",
       "3   NaN      True  2018-06-25  A11VN8EOHNLP72  B00FBJ4KYC   \n",
       "4   3.0      True  2016-08-17  A194Y8P8TVT7P9  B00P7G82TS   \n",
       "\n",
       "                                  style     reviewerName  \\\n",
       "0                {'Color:': ' FPS01-C'}    Brian C Toner   \n",
       "1  {'Size:': ' 1000W', 'Style:': ' G2'}            Snake   \n",
       "2                                   NaN  Amazon Customer   \n",
       "3                                   NaN      Jeremy Bray   \n",
       "4                                   NaN             Mark   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  The name and description of this device are mi...   \n",
       "1  One of the molex connectors on the power suppl...   \n",
       "2  Remote constantly disconnects/ Roku player fre...   \n",
       "3  I purchased this 4 year protection plan for a ...   \n",
       "4  I bought one of these and have regretted it ev...   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0  The prize for most useless invention of all ti...      1528070400  \n",
       "1                             MELTED MOLEX CONNECTOR      1518912000  \n",
       "2                             Bricked on the regular      1516406400  \n",
       "3                                      DO NOT BUY!!!      1529884800  \n",
       "4                              Nightmare - don't buy      1471392000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "print('shape of test_data : ',test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c115f099-e0d0-4488-8aac-2b3fbc20af62",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl;text-align: center;line-height:200%;font-family:vazir;font-size:medium;color:black\"><font face=\"vazir\" size=10><i>\r\n",
    "Feature Engineering\r\n",
    "</i></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769cba74-9dd2-49f6-b21c-b3d50f3f8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = train_data[['reviewText','overall']]\n",
    "new_test_data = test_data[['reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5952b2c1-64f1-4f33-9146-c761e5da9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before balancing : \n",
      "overall\n",
      "5    461485\n",
      "4    156514\n",
      "1     82950\n",
      "3     81239\n",
      "2     56756\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "after balancing : \n",
      "overall\n",
      "2    56756\n",
      "4    56756\n",
      "5    56756\n",
      "3    56756\n",
      "1    56756\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Value_Counts = new_train_data['overall'].value_counts()\n",
    "print(\"before balancing : \")\n",
    "print(Value_Counts ,end = '\\n\\n\\n---------------------------------------------\\n\\n\\n')\n",
    "\n",
    "#balance data\n",
    "'''\n",
    "reduce other samples to number of label 2 in dataset\n",
    "\n",
    "'''\n",
    "\n",
    "for i in range(1,6):\n",
    "    num_values_to_delete = Value_Counts[i] - Value_Counts[2]\n",
    "    rows_to_delete = new_train_data[new_train_data['overall']==i].sample(random_state=102 , n=num_values_to_delete)\n",
    "    \n",
    "    # Delete the selected rows from the DataFrame\n",
    "    new_train_data = new_train_data.drop(rows_to_delete.index)\n",
    "    \n",
    "\n",
    "Value_Counts = new_train_data['overall'].value_counts()\n",
    "print(\"after balancing : \")\n",
    "print(Value_Counts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f81648c-33a7-429f-ad41-b2268ee95ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary text :  This is not an example sentence for preprocessing.\n",
      "after applying changes  :  not example sentence preprocessing\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Removing punctuation\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens if re.sub(r'[^\\w\\s]', '', token)]\n",
    "    \n",
    "    # Delete negative words from stop words\n",
    "    negetive_words = {'aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\"doesn't\",'don',\"don't\",'few','hadn',\"hadn't\"\n",
    "                  ,'hasn',\"hasn't\", 'haven',\"haven't\", 'isn',\"isn't\", 'mightn',\"mightn't\",'mustn',\"mustn't\", 'needn',\"needn't\"\n",
    "                  , 'no','nor','not', 'shan',\"shan't\", 'shan',\"shan't\",'wasn',\"wasn't\",'weren',\"weren't\",\"won't\",'wouldn',\"wouldn't\",}\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words = stop_words.difference(negetive_words) \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Joining tokens back to text\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Example usage\n",
    "text = \"This is not an example sentence for preprocessing.\"\n",
    "print(\"primary text : \",text)\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print('after applying changes  : ',preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d9abaf-fdc8-4d25-b4cc-5ec0b0fb7bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    older urcwr7 remote thought would upgrade twc ...\n",
       "2    got 2 worked company called feedback told woul...\n",
       "6    90 degree connector bonus allows read charging...\n",
       "7    bought thinking would great replacement worn p...\n",
       "8    work skeptical ordering rating nt great short ...\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_data['reviewText']\n",
    "X = new_train_data['reviewText']\n",
    "y = new_train_data['overall']\n",
    "X_mapped = X.map(preprocess_text)\n",
    "X_test_mapped = X_test.map(preprocess_text)\n",
    "X_mapped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c445de-d932-4354-8df6-ed6f41f9a03c",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl;text-align: center;line-height:200%;font-family:vazir;font-size:medium;color:black\"><font face=\"vazir\" size=10><i>\n",
    "Model 1 : distilBERT\n",
    "</i></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cde4059-fc54-4eb9-90d3-b8a8aff3d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)cased/resolve/main/tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 18.4kB/s]\n",
      "(…)rt-base-uncased/resolve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 327kB/s]\n",
      "(…)bert-base-uncased/resolve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 823kB/s]\n",
      "(…)base-uncased/resolve/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.72MB/s]\n"
     ]
    }
   ],
   "source": [
    "#determin tokenizer to tokenize text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "#map tokenizer to train data\n",
    "tokenized_X = X_mapped.map(lambda x :tokenizer(x , truncation = True) )\n",
    "\n",
    "#map tokenizer to test data\n",
    "tokenized_test = X_test_mapped.map(lambda x :tokenizer(x , truncation = True) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#determin data collator \n",
    "'''\n",
    "Now create a batch of examples using DataCollatorWithPadding.\n",
    "It’s more efficient to dynamically pad the sentences to the \n",
    "longest length in a batch during collation, instead of padding\n",
    "the whole dataset to the maximum length.\n",
    "\n",
    "'''\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "# encode labels \n",
    "\n",
    "id2label = {0: 1, 1: 2 , 2:3 , 3:4 , 4:5 }\n",
    "label2id = {2: 1, 3: 2 , 4:3 , 5:4 , 1:0 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2c1b9-624d-4843-9738-c3fffce6d994",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c01795f6-ecc2-4338-b3fb-544d7cd9442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(tokenized_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ee2c2-d74d-4792-bccd-0ada67ff98f4",
   "metadata": {},
   "source": [
    "# preprocessing for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdcce147-19db-4737-9370-08772e2faba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 11087, 3435, 17371, 2224, 4077, 12935, 2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 3733, 16437, 3849, 2147, 2986, 2028, 357...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 4156, 13773, 24220, 10751, 23393, 2121, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 12511, 3641, 12200, 5916, 4470, 3274, 33...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2498, 6412, 3931, 3090, 4057, 8827, 2833...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227019</th>\n",
       "      <td>[101, 4950, 2843, 3465, 4950, 2525, 3030, 1824...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227020</th>\n",
       "      <td>[101, 4156, 1017, 2367, 3104, 2553, 2785, 2571...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227021</th>\n",
       "      <td>[101, 6046, 3715, 2099, 3369, 3835, 3482, 7218...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227022</th>\n",
       "      <td>[101, 3241, 4906, 19957, 18833, 3417, 3274, 20...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227023</th>\n",
       "      <td>[101, 4299, 3191, 4391, 4135, 23461, 3319, 272...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227024 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input_ids  label\n",
       "0       [101, 11087, 3435, 17371, 2224, 4077, 12935, 2...      4\n",
       "1       [101, 3733, 16437, 3849, 2147, 2986, 2028, 357...      3\n",
       "2       [101, 4156, 13773, 24220, 10751, 23393, 2121, ...      0\n",
       "3       [101, 12511, 3641, 12200, 5916, 4470, 3274, 33...      0\n",
       "4       [101, 2498, 6412, 3931, 3090, 4057, 8827, 2833...      1\n",
       "...                                                   ...    ...\n",
       "227019  [101, 4950, 2843, 3465, 4950, 2525, 3030, 1824...      1\n",
       "227020  [101, 4156, 1017, 2367, 3104, 2553, 2785, 2571...      4\n",
       "227021  [101, 6046, 3715, 2099, 3369, 3835, 3482, 7218...      3\n",
       "227022  [101, 3241, 4906, 19957, 18833, 3417, 3274, 20...      2\n",
       "227023  [101, 4299, 3191, 4391, 4135, 23461, 3319, 272...      0\n",
       "\n",
       "[227024 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split input_ids from attention_mask\n",
    "\n",
    "new_X_train = list()\n",
    "for i in X_train:\n",
    "    new_X_train.append(i['input_ids'])\n",
    "    \n",
    "\n",
    "#make dataframe from input_ids\n",
    "new_X_train = pd.DataFrame({'input_ids':new_X_train} )\n",
    "\n",
    "#set index and labels\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_train -= 1\n",
    "\n",
    "# make appropriate dataframe to turn into Tensor(input of the model)\n",
    "new_X_train['label'] = y_train\n",
    "\n",
    "#show \n",
    "new_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bd435-2224-4696-80aa-77f1915854c5",
   "metadata": {},
   "source": [
    "# preprocessing for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834e7983-752d-40b4-9d21-46f187959efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 5830, 2147, 2092, 2569, 2342, 5331, 5830...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2640, 8000, 9019, 3835, 3145, 3730, 3733...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 4031, 6412, 2360, 6047, 3104, 11892, 236...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 4540, 6806, 6559, 2015, 4540, 25101, 201...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2147, 2092, 3976, 2583, 8080, 4023, 2392...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56751</th>\n",
       "      <td>[101, 10463, 2121, 2235, 2092, 2328, 8702, 115...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56752</th>\n",
       "      <td>[101, 18833, 9949, 15581, 2121, 2147, 3819, 20...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56753</th>\n",
       "      <td>[101, 12087, 2500, 2478, 3298, 1019, 3204, 357...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56754</th>\n",
       "      <td>[101, 3835, 5549, 2213, 2525, 4642, 16086, 269...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56755</th>\n",
       "      <td>[101, 4149, 6445, 16048, 2651, 26963, 2581, 32...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56756 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_ids  label\n",
       "0      [101, 5830, 2147, 2092, 2569, 2342, 5331, 5830...      2\n",
       "1      [101, 2640, 8000, 9019, 3835, 3145, 3730, 3733...      2\n",
       "2      [101, 4031, 6412, 2360, 6047, 3104, 11892, 236...      1\n",
       "3      [101, 4540, 6806, 6559, 2015, 4540, 25101, 201...      0\n",
       "4      [101, 2147, 2092, 3976, 2583, 8080, 4023, 2392...      3\n",
       "...                                                  ...    ...\n",
       "56751  [101, 10463, 2121, 2235, 2092, 2328, 8702, 115...      4\n",
       "56752  [101, 18833, 9949, 15581, 2121, 2147, 3819, 20...      4\n",
       "56753  [101, 12087, 2500, 2478, 3298, 1019, 3204, 357...      1\n",
       "56754  [101, 3835, 5549, 2213, 2525, 4642, 16086, 269...      2\n",
       "56755  [101, 4149, 6445, 16048, 2651, 26963, 2581, 32...      0\n",
       "\n",
       "[56756 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_val = list()\n",
    "for i in X_val:\n",
    "    new_X_val.append(i['input_ids'])\n",
    "    \n",
    "\n",
    "#make dataframe from input_ids\n",
    "new_X_val = pd.DataFrame({'input_ids':new_X_val} )\n",
    "\n",
    "#set index and labels\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "y_val -= 1\n",
    "\n",
    "# make appropriate dataframe to turn into Tensor(input of the model)\n",
    "new_X_val['label'] = y_val\n",
    "\n",
    "#show \n",
    "new_X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ae1ae-b125-4638-95ab-47bc29b9b4c6",
   "metadata": {},
   "source": [
    "# preprocessing for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f4d7d1b-440e-4f39-8169-9d59e5d3ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2171, 6412, 5080, 22369, 4011, 7584, 118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2028, 16709, 2595, 19400, 2373, 4425, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 6556, 7887, 12532, 10087, 6593, 20996, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 4156, 1018, 2095, 3860, 2933, 3036, 2291...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 4149, 2028, 18991, 2412, 2144, 14658, 83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>[101, 2770, 1017, 2154, 2521, 3492, 7537, 9725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>[101, 2559, 2152, 3737, 5746, 6970, 8663, 2638...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>[101, 3819, 3599, 2409, 5830, 2009, 9286, 2460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>[101, 4149, 12418, 15250, 1041, 2575, 22394, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>[101, 4487, 2615, 8909, 2678, 23467, 2099, 169...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_ids\n",
       "0      [101, 2171, 6412, 5080, 22369, 4011, 7584, 118...\n",
       "1      [101, 2028, 16709, 2595, 19400, 2373, 4425, 23...\n",
       "2      [101, 6556, 7887, 12532, 10087, 6593, 20996, 5...\n",
       "3      [101, 4156, 1018, 2095, 3860, 2933, 3036, 2291...\n",
       "4      [101, 4149, 2028, 18991, 2412, 2144, 14658, 83...\n",
       "...                                                  ...\n",
       "19995  [101, 2770, 1017, 2154, 2521, 3492, 7537, 9725...\n",
       "19996  [101, 2559, 2152, 3737, 5746, 6970, 8663, 2638...\n",
       "19997  [101, 3819, 3599, 2409, 5830, 2009, 9286, 2460...\n",
       "19998  [101, 4149, 12418, 15250, 1041, 2575, 22394, 2...\n",
       "19999  [101, 4487, 2615, 8909, 2678, 23467, 2099, 169...\n",
       "\n",
       "[20000 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = list()\n",
    "for i in tokenized_test:\n",
    "    new_test.append(i['input_ids'])\n",
    "    \n",
    "\n",
    "#make dataframe from input_ids\n",
    "new_test = pd.DataFrame({'input_ids':new_test} )\n",
    "\n",
    "#show \n",
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39c7b3-ab2d-44ae-9b8a-eb3fea567a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee901ed-97b2-474d-b0e5-37cf61e3b363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a415aca8-2e33-4223-ba0a-3b77086a8fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 12:36:42.899190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:42.904282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:42.904545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:42.907445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:42.907656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:42.907839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:44.576935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:44.577191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:44.577386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 12:36:44.577546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21739 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:10<00:00, 26.1MB/s] \n",
      "2023-11-12 12:36:57.496994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(X_train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "\n",
    "# create model\n",
    "\n",
    "model1 = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                            num_labels=5, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36288c12-437c-471a-a70b-11e90c491814",
   "metadata": {},
   "source": [
    "# turn datasets into appropriate form for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3020dc21-52e7-44c3-be16-a785d3b5688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(new_X_train)\n",
    "val_dataset = Dataset.from_pandas(new_X_val)\n",
    "test_dataset = Dataset.from_pandas(new_test)\n",
    "\n",
    "\n",
    "tf_train_set = model1.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model1.prepare_tf_dataset(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "tf_test_set = model1.prepare_tf_dataset(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c66c180-0aa8-4324-b231-035f772f6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and set some hyperparameters\n",
    "model1.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=10, verbose=1,restore_best_weights=True,min_delta=0.001)\n",
    "reduce_lr = tf._keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5ab2c-6eda-478a-a9a4-de14f8589891",
   "metadata": {},
   "source": [
    "# Fit model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52ba15ef-7e1c-4e72-a2ec-51b905425aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 12:37:56.000135: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16fe38710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-12 12:37:56.000169: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-11-12 12:37:56.004647: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-12 12:37:56.144479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
      "2023-11-12 12:37:56.198958: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-11-12 12:37:56.267676: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28377/28378 [============================>.] - ETA: 0s - loss: 1.0214 - accuracy: 0.5579\n",
      "Epoch 1: val_accuracy improved from -inf to 0.58131, saving model to best_model.h5\n",
      "28378/28378 [==============================] - 1694s 59ms/step - loss: 1.0214 - accuracy: 0.5579 - val_loss: 0.9556 - val_accuracy: 0.5813 - lr: 1.6000e-05\n",
      "Epoch 2/3\n",
      "28378/28378 [==============================] - ETA: 0s - loss: 0.8578 - accuracy: 0.6319\n",
      "Epoch 2: val_accuracy improved from 0.58131 to 0.60247, saving model to best_model.h5\n",
      "28378/28378 [==============================] - 1507s 53ms/step - loss: 0.8578 - accuracy: 0.6319 - val_loss: 0.9343 - val_accuracy: 0.6025 - lr: 1.2000e-05\n",
      "Epoch 3/3\n",
      "28378/28378 [==============================] - ETA: 0s - loss: 0.7290 - accuracy: 0.6896\n",
      "Epoch 3: val_accuracy did not improve from 0.60247\n",
      "28378/28378 [==============================] - 1512s 53ms/step - loss: 0.7290 - accuracy: 0.6896 - val_loss: 0.9751 - val_accuracy: 0.6018 - lr: 8.0001e-06\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3 , batch_size = 8 ,callbacks=[early_stopping, model1_checkpoint,reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ad0bb-4433-43df-b4c7-e8344ecdb1cc",
   "metadata": {},
   "source": [
    "# Evaluation on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1dabe1c-5a00-4f15-ba64-9b88435857fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7095/7095 [==============================] - 141s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict(tf_validation_set)\n",
    "pred1 = tf.math.argmax(pred1.logits, axis=-1)\n",
    "pred1 = np.array(pred1)\n",
    "y_val += 1\n",
    "pred1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b3d109d-18a4-485f-8312-d3b23c9b22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.65      0.68     12619\n",
      "           2       0.54      0.48      0.51     12707\n",
      "           3       0.45      0.52      0.49      9845\n",
      "           4       0.55      0.61      0.58     10353\n",
      "           5       0.74      0.75      0.74     11232\n",
      "\n",
      "    accuracy                           0.60     56756\n",
      "   macro avg       0.60      0.60      0.60     56756\n",
      "weighted avg       0.61      0.60      0.60     56756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred1 , y_val ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7550c8-b867-4783-bac3-8a5780bd8fcb",
   "metadata": {},
   "source": [
    "# prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a057a8b4-d87a-4162-9925-19f84e396234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 50s 20ms/step\n",
      "[1 1 1 ... 5 2 4]\n"
     ]
    }
   ],
   "source": [
    "pred1_test = model1.predict(tf_test_set)\n",
    "pred1_test = tf.math.argmax(pred1_test.logits, axis=-1)\n",
    "pred1_test = np.array(pred1_test)\n",
    "pred1_test += 1\n",
    "\n",
    "print(pred1_test)\n",
    "\n",
    "#to csv\n",
    "pred1_test = pd.DataFrame(pred1_test,columns=['predicted'])\n",
    "pred1_test.to_csv('q2_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066e7fd-b919-40eb-bece-238fe62d4e5b",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1899630f-950d-490e-b9d3-b67af863d19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f970c48c6d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f970c5235b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f970c46c550>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f970c4774f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f970c406490>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f970c414430>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: model1_distilBERT/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model1_distilBERT/assets\n"
     ]
    }
   ],
   "source": [
    "model1.save('model1_distilBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d6891-b527-4c65-a026-518c2c832e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
